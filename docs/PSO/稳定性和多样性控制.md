# 稳定性与多样性控制
# 现有资料总结与相应知识点对照

***

# 1 标准PSO算法

（1）速度更新公式

$$
V^{t+1}_{id}=w·V^{t}_{id}+c_1·r_1·(Pbest^t_{id}-X_{id}^t)+c_2·r_2(Gbest^t_{id}-X^t_{id})
$$


（2）位置更新公式
$$
X_{id}^{t+1}=X^t_{id}+V^{t+1}_{id}
$$


# 2 胡老师论文中的SPSO-SIS

## 2.1 升级策略

为了建立一个性能测试的基准, 文献把带压缩因子PSO规定为标准 PSO (Standard PSO, 由于其是在 Proceedings of the 2007 IEEE Swarm Intelligence Symposium 上被提出来的, 本文简称之为 SPSO-SIS), 其速度和位置更新规则分别为式 (1) 和 (2):

（1）速度更新公式

$$
V_{i,d}=\chi·[V_{i,d}+\frac{\phi}{2}·rand_1·(P_{i,d}-X_{i,d})+\frac{\phi}{2}·rand_2·(P_{n,d}-X_{i,d})]  (1)
$$
（2）位置更新公式

$$
X_{i,d}=X_{i,d}+V_{i,d}  (2)
$$
其中, 下标$i$和$d$分别表示粒子和维度的序号； $V_{i,d}$ 和 $X_{i,d}$ 分别表示粒子$i$在$d$维上的速度和位置; $P_{i,d}$ 表示粒子$i$的历史最优位置的第$d$维；$n$表示邻居中最优粒子的序号； $P_{n,d}$ 表示邻居的历史最优位置的第$d$维；$\chi$是压缩因子, 取近似值 0.72984; $\phi$是常数, 取值4.1；rand是服从均匀分布的范围为$[0, 1]$的随机数, 不同的下标表示随机数相互独立. 为了方便起见, 把式 (1) 和 (2) 合并为一个位置更新方程, 并去掉部分下标$i$和$d$, 标上循环次数$t$, 即有:
$$
X(t)=X(t-1)+\chi·\{V(t-1)+\frac{\phi}{2}·rand_1·[P_i-X(t-1)]+\frac{\phi}{2}·rand_2·[P_n-X(t-1)]\}  (3)
$$
由于PSO算法隐喻着社会心理现象，研究者们通常把压缩因子称为惯性因子（记为$\omega$），把式子（3）展开后的$P_i-X(t-1)$的系数$\chi·\frac{\phi}{2}$称为认知因子（记为$C_1$），$P_n-X(t-1)$的系数$\chi·\frac{\phi}{2}$称为社会因子（记为$C_2$），于是式（3）可改写为：
$$
X(t)=X(t-1)+\omega·V(t-1)+C_1·rand_1·[P_i-X(t-1)]+C_2·rand_2·[P_n-X(t-1)]  (4)
$$

***

**其实到这里就可以看出，上式已经化为标准PSO的位置更新式，只是把**$V_{id}^{t+1}$**的展开式代入了而已，所以1 标准PSO算法 和 2 胡老师论文中的SPSO-SIS的更新规则是完全一样的，这里进行罗列是为了记录推导过程（已完成笔算推导）**

***

## 2.2 位置更新方程的统一形式

**这一节主要对胡老师论文中的统一形式进行总结，由于选取的对象是CLPSO，所以下文主要对论文中的“第1类算法”总结。**

这里先介绍**CLPSO算法**：Comprehensive Learning Particle Swarm Optimizer，其思想是认为把邻居粒子的最优位置和自身历史最优位置**按维（关键）**进行组合，可以形成更优秀的榜样（标识为$P$），位置更新方程为：

$$
X(t)=X(t-1)+\omega·V(t-1)+C·rand·[P-X(t-1)]
$$
其中$C$是系数，在$P$的确定过程中，CLPSO采用的是在邻域中随机择优的方式。

第1类算法是指：隐喻社会心理学中的学习思想，增减或修改位置更新方程中的部分项。

第2类算法是指：抛弃或部分抛弃社会心理学中的学习思想，采用概率或物理现象模拟位置更新的方法。

**==位置更新方程的统一式：==**
$$
X(t)=\sum\limits_{j=1,2}a_j·X(t-j)+\sum\limits_{k=i,n}b_k·P_k
$$
其中，$a_1=1+\omega-C_1·rand_1-C_2·rand_2$，$a_2=-\omega$，$b_i=C_1·rand_1$，$b_n=C_2·rand_2$。

根据上述统一式的倒推，可以得到下式（注意，这里的上标和前文式子中的括号标意义相同）：

$$
X^t=X^{t-1}+\omega·(X^{t-1}-X^{t-2})+C·rand·(P-X^{t-1})
$$
跟CLPSO算法的位置更新方程进行对比，可以发现存在差异的部分仅有：$\omega·V(t-1)$与$\omega·(X^{t-1}-X^{t-2})$，注意，这里将$C_1·rand_1$和$C_2·rand_2$合并为了$C·rand$

==这部分重新推理一下吧==

## 2.3收敛性控制

## 2.4群体聚集的控制

## 2.5实验

（1）备选算法

*   Evolutionary Algorithm

&#x9;			Genetic Algorithm

&#x9;			Evolutionary Programming

&#x9;			Evolutionary Strategy

&#x9;			Genetic Programming

*   Swarm Intelligence

&#x9;			Ant Colony Optimization

&#x9;			Bacterial Foraging Algorithms

&#x9;			Brain Storm Optimization Algorithms

&#x9;			Cultural Algorithms

&#x9;			Fish School Optimization

&#x9;			Particle Swarm Optimization

&#x9;			etc.

（2）各算法位置更新方程及统一式

*   SPSO-SIS (Standard PSO)

位置更新方程

$$
X^t=X^{t-1}+W·(X^{t-1}-X^{t-2})+C_1·rand_1·(L-X^{t-1})+C_2·rand_2·(P-X^{t-1})
$$
统一式

$$
X^t=Q+(1+W-C_1·rand_1-C_2·rand_2)·(X^{t-1}-Q)+(-W)·(X^{t-2}-Q)
$$

$$
Q=\frac{C_1·rand_1·L+C_2·rand_2·P}{C_1·rand_1+C_2·rand_2}
$$


*   CLPSO (Comprehensive Learning PSO）

位置更新方程

$$
X^t=X^{t-1}+W·(X^{t-1}-X^{t-2})+C·rand·(P-X^{t-1})
$$


统一式

$$
X^t=Q+(1+W-C·rand)·(X^{t-1}-Q)+(-W)·(X^{t-2}-Q)
$$

$$
Q=P
$$


*   OLPSO (Orthogonal Learning PSO)

位置更新方程

$$
X^t=X^{t-1}+W·(X^{t-1}-X^{t-2})+C·rand·(P-X^{t-1})
$$


统一式

$$
X^t=Q+(1+W-C·rand)·(X^{t-1}-Q)+(-W)·(X^{t-2}-Q)
$$

$$
Q=P
$$


*   SCA (Sine Cosine Algorithm, 正余弦算法)
*   GWO (Grey Wolf Optimizer, 灰狼优化算法)
*   ALO (Ant Lion Optimizer, 蚁狮优化算法)
*   MVMO (Mean- Variance Mapping Optimization)
*   FOA (Forest Optimization Algorithm)
*   TLBO (Teaching-learning-based Optimization)
*   DE (Differential Evolution)

# 3 意义

（1）自适应调节稳定性和多样性

虽然有些文献采用重新初始化等策略，但是仍未根本解决问题，因为维之间不是同时重新初始化，即便同时重新初始化，又会进入新的收敛不均。因此，需要在进化过程中进行调节。虽然也可以通过控制原有参数等来调节各维的收敛性，但是参数过多，不易调节。

因此，本文采用简化统一的位置升级式来调节收敛性：收敛性大的维取小的C，反之取大C。 考虑到C并不会立即影响该维的收敛性，故反复升级。

（2）各种调节及优缺点

| 调节方式  | 优点                       | 缺点                                                                                                                      | 备注 |
| :---- | :----------------------- | :---------------------------------------------------------------------------------------------------------------------- | :- |
| 群体收敛性 | （1）控制群体的整体速度达到期望值        | （1）按倍数在控制，在所有维上的平均速度大的粒子运动得更大，各粒子之间变得更为不均匀；（2）按倍数在控制，在所有粒子上的平均速度大的维运动得更大，各维间变得更为不均匀；（3）无法控制群体聚集；（4）当期望收敛性很小时，粒子较难向榜样学习。 |    |
| 粒子收敛性 | （1）控制各粒子在所有维上平均速度均达到期望值  | 上面缺点（2）、（3）、（4）                                                                                                         |    |
| 维收敛性  | （1）控制各维在所有粒子上的平均速度均达到期望值 | 上面缺点（1）、（3）、（4）                                                                                                         |    |

（3）其他想法

详细笔记待完善

# 4 20190606-（代码在MyCodes20190606）

==无需补充“范数”的知识点==

